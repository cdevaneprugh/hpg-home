<?xml version="1.0"?>
<config_machines version="2.0">
  <machine MACH="hipergator">
    <DESC>HiPerGator testing on the hpg-dev node</DESC>

    <!--should pick up any of the nodes on login, hpg-dev, and hpg-default-->
    <NODENAME_REGEX>c07*|login*</NODENAME_REGEX>
    
    <OS>LINUX</OS>
    <COMPILERS>gnu,intel</COMPILERS>
    <MPILIBS>openmpi,impi,mpi-serial</MPILIBS>
    
    <!--input and output for cases, boundary conditions, etc-->
    <CIME_OUTPUT_ROOT>/blue/gerber/cdevaneprugh/cesm_data_root/cime_output_root</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/blue/gerber/cdevaneprugh/cesm_data_root/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/blue/gerber/cdevaneprugh/cesm_data_root/lmwg</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/blue/gerber/cdevaneprugh/cesm_data_root/cesm_baselines</BASELINE_ROOT>
    
    <!-- not sure if I even need to set this. default is gmake->make. 
         GMAKE_J: optional number of threads to pass to the gmake flag -->
    <GMAKE>make</GMAKE>
<!--<GMAKE_J>8</GMAKE_J> look up what default is-->

<!--for testing on hpg-dev, disable batch system-->
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>cdevaneprugh</SUPPORTED_BY>

    <!--CURRENTLY SET FOR HPG-DEV. FOR HPG-DEFAULT SET TO 128-->
    <!-- MAX_TASKS_PER_NODE: maximum number of threads*tasks per
	 shared memory node on this machine,
	 should always be >= MAX_MPITASKS_PER_NODE -->
    <MAX_TASKS_PER_NODE>64</MAX_TASKS_PER_NODE>

    <!-- MAX_MPITASKS_PER_NODE: number of physical PES per shared node on
	 this machine, in practice the MPI tasks per node will not exceed this value -->
    <MAX_MPITASKS_PER_NODE>64</MAX_MPITASKS_PER_NODE>
    
<!-- not sure if this should be blank or set to false for testing on hpg-dev-->
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
   
<!--can add extra arguments if needed. ex: tasks = -np {number of tasks}--> 
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="ntasks"> -np {{ total_tasks }} </arg>
      </arguments>
    </mpirun>

<!--this seems weird to set with no executable, other machines had something 
     similar. comment out if it's a problem-->
<!--
    <mpirun mpilib="mpi-serial">
      <executable></executable>
    </mpirun>
-->

<!--I'm pretty sure all this is correct-->    
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/apps/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/apps/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="csh">/apps/lmod/lmod/init/csh</init_path>
      <init_path lang="sh">/apps/lmod/lmod/init/sh</init_path>
      <cmd_path lang="perl">/apps/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/apps/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

 <!-- Not sure how they want us to load modules. It looks like
        most other configs purge the system then load in the needed
	modules for the case. I'll try to restore saved collections for now -->
      <modules>
	<command name="purge"/>
	<command name="restore">cesm_gcc_testing</command>
      </modules>
    </module_system>

    <environment_variables>

      <!-- based on size of L3 cache -->
      <env name="OMP_STACKSIZE">256M</env>

    </environment_variables>

    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>

  </machine>
</config_machines>
