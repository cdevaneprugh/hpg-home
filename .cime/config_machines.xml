<?xml version="1.0"?>
<config_machines version="2.0">
  <machine MACH="hipergator">
    <DESC>HiPerGator | hpg-default node | AMD ROME 128 cores per node | slurm</DESC>

    <!--regex matches login, hpg-dev, and hpg-default nodes-->
    <NODENAME_REGEX>c07*|login*</NODENAME_REGEX>
   
    <!--Red Hat Enterprise Linux 8.8 (Ootpa)--> 
    <OS>LINUX</OS>

    <!--comment separated with default first-->
    <COMPILERS>gnu,intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
   
    <!--USER DEFINED FILE IO. USers should only need to change the group name in the following settings as well as the location of the cprnc executable.-->

    <!--for e3sm only--> 
    <SAVE_TIMING_DIR>/blue/gerber/$ENV{USER}/earth_model_output/timings</SAVE_TIMING_DIR>
    
    <!--root directory for case output. should be in the user's /blue storage, NOT in shared space-->
    <CIME_OUTPUT_ROOT>/blue/gerber/$ENV{USER}/earth_model_output/cime_output_root</CIME_OUTPUT_ROOT>
    
    <!--input data should be shared between all users in the group-->
    <DIN_LOC_ROOT>/blue/gerber/earth_models/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$DIN_LOC_ROOT/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    
    <!--short term archive files-->
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    
    <!--system test baseline files-->
    <BASELINE_ROOT>/blue/gerber/$ENV{USER}/earth_model_output/cesm_baselines</BASELINE_ROOT>
    
    <!--cprnc executable location, built with gcc/12.2.0 may not work with intel built experiments-->
    <CCSM_CPRNC>/blue/gerber/earth_models/cprnc/bld/cprnc</CCSM_CPRNC>

    <!--END USER DEFINED FILE IO-->

    <!--make program. can also add number of threads to pass to make with GMAKE_J tag (optional)-->
    <GMAKE>make</GMAKE>

    <!--for scheduler documentation: https://slurm.schedmd.com/documentation.html-->
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>

    <SUPPORTED_BY>cdevaneprugh@ufl.edu</SUPPORTED_BY>

    <!--number of cores per node on hpg-default partition-->
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE> <!--MAX_TASKS_PER_NODE >= MAX_MPITASKS_PER_NODE-->
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
   
    <!--project name not required for hpg, usually used in systems for account billing-->
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
   
    <!--can add extra arguments if needed. ex: tasks = -np {number of tasks} or something similar--> 
    <mpirun mpilib="default">
      <executable>mpirun</executable>
    </mpirun>

    <!--lmod system paths and module load order-->
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/apps/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/apps/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="csh">/apps/lmod/lmod/init/csh</init_path>
      <init_path lang="sh">/apps/lmod/lmod/init/sh</init_path>
      <cmd_path lang="perl">/apps/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/apps/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

    <!--Load for both compilers-->
    <modules>
      <command name="load">cmake/3.26.4</command>
      <command name="load">perl/5.24.1</command>
    </modules>

    <!--Load for gnu compiler-->
    <modules compiler="gnu">
      <command name="load">gcc/12.2.0</command>
      <command name="load">gcc/12.2.0 lapack/3.11.0</command>
      <command name="load">gcc/12.2.0 openmpi/4.1.5</command>
      <command name="load">gcc/12.2.0 openmpi/4.1.5 python/3.11</command>
      <command name="load">gcc/12.2.0 openmpi/4.1.5 netcdf-c/4.9.2</command>
      <command name="load">gcc/12.2.0 openmpi/4.1.5 netcdf-f/4.6.1</command>
    </modules>

    <!--Load for intel compiler-->
    <modules compiler="intel">
      <command name="load">intel/2020.0.166</command>
      <command name="load">intel/2020.0.166 python/3.11.4</command>
      <command name="load">intel/2020.0.166 openmpi/4.1.5</command>
      <command name="load">intel/2020.0.166 openmpi/4.1.5 netcdf-c/4.9.2</command>
      <command name="load">intel/2020.0.166 openmpi/4.1.5 netcdf-f/4.6.1</command>
    </modules>

    </module_system>

    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env><!--L3 cache size of hpg-default cpu-->
    </environment_variables>

    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource><!--python resource limit-->
    </resource_limits>

  </machine>
</config_machines>
