<?xml version="1.0"?>
<config_machines version="2.0">
  <machine MACH="hipergator">
    <DESC>HiPerGator testing on the hpg-dev node</DESC>

    <!--should pick up any of the nodes on login, hpg-dev, and hpg-default-->
    <NODENAME_REGEX>c07*|login*</NODENAME_REGEX>
    
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
 
    <!--input and output for cases, boundary conditions, etc-->
    <CIME_OUTPUT_ROOT>/blue/gerber/cdevaneprugh/cesm_data_root/cime_output_root</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/blue/gerber/cdevaneprugh/cesm_data_root/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/blue/gerber/cdevaneprugh/cesm_data_root/lmwg</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/blue/gerber/cdevaneprugh/cesm_data_root/cesm_baselines</BASELINE_ROOT>
    
    <GMAKE>make</GMAKE>

    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>cdevaneprugh</SUPPORTED_BY>

    <!-- MAX_TASKS_PER_NODE: maximum number of threads*tasks per
	 shared memory node on this machine,
	 should always be >= MAX_MPITASKS_PER_NODE -->
    <MAX_TASKS_PER_NODE>10</MAX_TASKS_PER_NODE>

    <!-- MAX_MPITASKS_PER_NODE: number of physical PES per shared node on
	 this machine, in practice the MPI tasks per node will not exceed this value -->
    <MAX_MPITASKS_PER_NODE>10</MAX_MPITASKS_PER_NODE>
    
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
   
<!--can add extra arguments if needed. ex: tasks = -np {number of tasks}--> 
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="ntasks"> -np {{ total_tasks }} </arg>
      </arguments>
    </mpirun>

<!--this seems weird to set with no executable, but other machines had something 
     similar.-->
<!--
    <mpirun mpilib="mpi-serial">
      <executable></executable>
    </mpirun>
-->

<!--I'm pretty sure all this is correct-->    
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/apps/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/apps/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="csh">/apps/lmod/lmod/init/csh</init_path>
      <init_path lang="sh">/apps/lmod/lmod/init/sh</init_path>
      <cmd_path lang="perl">/apps/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/apps/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

 <!-- Not sure how they want us to load modules. It looks like
        most other configs purge the system then load in the needed
	modules for the case. I'll try to restore saved collections for now -->
      <modules>
	<command name="purge"/>
	<command name="restore">cesm_gcc_testing</command>
      </modules>
    </module_system>

    <environment_variables>

      <!-- based on size of L3 cache of hpg-default processors-->
      <env name="OMP_STACKSIZE">256M</env>

    </environment_variables>

    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>

  </machine>
</config_machines>
