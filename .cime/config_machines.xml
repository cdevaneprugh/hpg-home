<?xml version="1.0"?>
<config_machines version="2.0">
  <machine MACH="hipergator">
    <DESC>HiPerGator | AMD ROME 128 cores per node | slurm | lmod</DESC>

    <!--matches nodes on login, hpg-dev, and hpg-default partitions-->
    <NODENAME_REGEX>c07*|login*</NODENAME_REGEX>
   
    <!--Red Hat Enterprise Linux 8.8 (Ootpa)--> 
    <OS>LINUX</OS>

    <!--comment separated with default first-->
    <COMPILERS>gnu,intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
 
    <!--root directory for case output. should be in the user's /blue storage space-->
    <CIME_OUTPUT_ROOT>/blue/gerber/$ENV{USER}/earth_model_output/cime_output_root</CIME_OUTPUT_ROOT>
    
    <!--input data should be shared between all users in the group-->
    <DIN_LOC_ROOT>/blue/gerber/earth_models/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$DIN_LOC_ROOT/clmforce</DIN_LOC_ROOT_CLMFORC>
    
    <!--short term archive files-->
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    
    <!--system test baseline files-->
    <BASELINE_ROOT>/blue/gerber/$ENV{USER}/earth_model_output/cesm_baselines</BASELINE_ROOT>
    
    <!--cprnc executable location-->
    <CCSM_CPRNC>/blue/gerber/earth_models/cesm215/cime/tools/cprnc/bld/cprnc</CCSM_CPRNC>

    <!--can also add number of threads to pass to make with GMAKE_J tag (optional)-->
    <GMAKE>make</GMAKE>

    <!--for scheduler documentation: https://slurm.schedmd.com/documentation.html-->
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>

    <SUPPORTED_BY>sgerber</SUPPORTED_BY>

    <!--set to number of cores available in your QOS-->
    <MAX_TASKS_PER_NODE>10</MAX_TASKS_PER_NODE> <!--MAX_TASKS_PER_NODE >= MAX_MPITASKS_PER_NODE-->
    <MAX_MPITASKS_PER_NODE>10</MAX_MPITASKS_PER_NODE>
   
    <!--project name not required for hpg, usually used in systems for account billing-->
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
   
    <!--can add extra arguments if needed. ex: tasks = -np {number of tasks} or something similar--> 
    <mpirun mpilib="default">
      <executable>mpirun</executable>
    </mpirun>

    <!--lmod system paths and module load order-->
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/apps/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/apps/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="csh">/apps/lmod/lmod/init/csh</init_path>
      <init_path lang="sh">/apps/lmod/lmod/init/sh</init_path>
      <cmd_path lang="perl">/apps/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/apps/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

    <!--restores collection of modules, may choose to load individual ones later-->
    <modules compiler="gnu">
      <command name="restore">gnu_env</command>
    </modules>

    <modules compiler="intel">
      <command name="restore">intel_env</command>
    </modules>

    </module_system>

    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env> <!--L3 cache size of hpg-default cpu-->
    </environment_variables>

    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>

  </machine>
</config_machines>
